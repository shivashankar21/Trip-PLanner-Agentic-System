{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18772359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0255cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d928c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Groq API with different models to find available ones\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# List of models to test\n",
    "models_to_test = [\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    \"llama-3.2-90b-vision-preview\",\n",
    "    \"llama-3.2-11b-vision-preview\",\n",
    "    \"llama-3.2-3b\",\n",
    "    \"llama-3.1-70b-versatile\",\n",
    "    \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b-it\",\n",
    "    \"gemma2-9b-it\",\n",
    "    \"llama-3.3-8b-instant\",\n",
    "]\n",
    "\n",
    "print(\"Testing Groq models to find available ones...\\n\")\n",
    "available_models = []\n",
    "decommissioned_models = []\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    try:\n",
    "        print(f\"Testing {model_name}...\", end=\" \")\n",
    "        llm = ChatGroq(model=model_name, temperature=0)\n",
    "        response = llm.invoke(\"Say 'OK' if you can hear me\")\n",
    "        print(f\"âœ“ AVAILABLE\")\n",
    "        available_models.append(model_name)\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"decommissioned\" in error_msg.lower():\n",
    "            print(f\"âœ— DECOMMISSIONED\")\n",
    "            decommissioned_models.append(model_name)\n",
    "        else:\n",
    "            print(f\"âœ— ERROR: {error_msg[:50]}...\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"AVAILABLE MODELS: {len(available_models)}\")\n",
    "for model in available_models:\n",
    "    print(f\"  âœ“ {model}\")\n",
    "    \n",
    "print(f\"\\nDECOMMISSIONED MODELS: {len(decommissioned_models)}\")\n",
    "for model in decommissioned_models:\n",
    "    print(f\"  âœ— {model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f4982",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'env (Python 3.10.18)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/chith/OneDrive/Documents/GitHub/AI_Trip_Planner/env/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Test a specific model (update model_name after running the test above)\n",
    "# Use the first available model from the list above\n",
    "if available_models:\n",
    "    test_model = available_models[0]\n",
    "    print(f\"Testing {test_model} with a sample query...\\n\")\n",
    "    \n",
    "    llm = ChatGroq(model=test_model, temperature=0)\n",
    "    response = llm.invoke(\"What is 2+2? Answer in one sentence.\")\n",
    "    print(f\"Response: {response.content}\")\n",
    "    print(f\"\\nâœ“ Model {test_model} is working correctly!\")\n",
    "    print(f\"\\nRecommended model for config.yaml: {test_model}\")\n",
    "else:\n",
    "    print(\"No available models found. Please check your GROQ_API_KEY or try different model names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f85ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'env (Python 3.10.18)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/chith/OneDrive/Documents/GitHub/AI_Trip_Planner/env/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Optionally: Update config.yaml with the first available model\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "\n",
    "if available_models:\n",
    "    recommended_model = available_models[0]\n",
    "    config_path = \"../config/config.yaml\"\n",
    "    \n",
    "    # Read current config\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Update Groq model\n",
    "    old_model = config['llm']['groq']['model_name']\n",
    "    config['llm']['groq']['model_name'] = recommended_model\n",
    "    \n",
    "    # Write back\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    print(f\"âœ“ Updated config.yaml:\")\n",
    "    print(f\"  Old model: {old_model}\")\n",
    "    print(f\"  New model: {recommended_model}\")\n",
    "    print(f\"\\nPlease restart your uvicorn server for changes to take effect.\")\n",
    "else:\n",
    "    print(\"No available models to update config with.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd425c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: Test a single model (modify model_name as needed)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Change this to test different models\n",
    "test_model_name = \"llama-3.1-8b-instant\"  # Try: llama-3.1-8b-instant, llama-3.3-70b-versatile, etc.\n",
    "\n",
    "try:\n",
    "    print(f\"Testing model: {test_model_name}\\n\")\n",
    "    llm = ChatGroq(model=test_model_name, temperature=0)\n",
    "    response = llm.invoke(\"Hello! Please respond with 'Model is working' if you can hear me.\")\n",
    "    print(f\"âœ“ SUCCESS!\")\n",
    "    print(f\"Response: {response.content}\")\n",
    "    print(f\"\\nThis model is available and can be used in config.yaml\")\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if \"decommissioned\" in error_msg.lower():\n",
    "        print(f\"âœ— Model {test_model_name} has been DECOMMISSIONED\")\n",
    "        print(f\"Error: {error_msg}\")\n",
    "    else:\n",
    "        print(f\"âœ— Error testing model: {error_msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c38f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nHello! How can I assist you today? ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 4, 'total_tokens': 20, 'completion_time': 0.084725173, 'prompt_time': 6.0159e-05, 'queue_time': 0.053524741, 'total_time': 0.084785332}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'finish_reason': 'stop', 'logprobs': None}, id='run--8b091ea3-0a7d-4058-9e8b-2a68094cc078-0', usage_metadata={'input_tokens': 4, 'output_tokens': 16, 'total_tokens': 20})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ac345",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a (int): The first integer.\n",
    "        b (int): The second integer.\n",
    "\n",
    "    Returns:\n",
    "        int: The product of a and b.\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79ae97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='multiply', description='Multiply two integers.\\n\\nArgs:\\n    a (int): The first integer.\\n    b (int): The second integer.\\n\\nReturns:\\n    int: The product of a and b.', args_schema=<class 'langchain_core.utils.pydantic.multiply'>, func=<function multiply at 0x000002DFC40C3A30>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2927babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import StructuredTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03483480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherInput(BaseModel):\n",
    "    city: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the weather for a given city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city.\n",
    "\n",
    "    Returns:\n",
    "        str: A string describing the weather in the city.\n",
    "    \"\"\"\n",
    "    return f\"The weather in {city} is sunny.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45850a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_tool = StructuredTool.from_function(\n",
    "    func=get_weather,\n",
    "    name=\"get_weather\",\n",
    "    description=\"Fetches real-time weather data for a city\",\n",
    "    args_schema=WeatherInput,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ee4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherInput(BaseModel):\n",
    "    city: str = Field(..., description=\"City name\")\n",
    "    units: str = Field(\"metric\", description=\"metric or imperial\")\n",
    "\n",
    "class GetWeatherTool(StructuredTool):\n",
    "    name: ClassVar[str] = \"get_weather\"           \n",
    "    description: ClassVar[str] = (\n",
    "        \"Fetches weather data for a city\"\n",
    "    )\n",
    "    args_schema: ClassVar[Type[BaseModel]] = WeatherInput\n",
    "\n",
    "    def _run(self, city: str, units: str = \"metric\") -> str:\n",
    "        return get_weather(city, units)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
